{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kinase_library as kl\n",
    "testsequence='PSVEPPLsQETFSDL'\n",
    "\n",
    "# Create a Substrate object with a target sequence (example: p53 S33)\n",
    "s = kl.Substrate(testsequence)  # Lowercase 's' indicates a phosphoserine\n",
    "\n",
    "# Predict potential kinase interactions for the substrate\n",
    "s.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1 - to write code mutating first amino acid to every other possible amino acid\"\"\"\n",
    "#how to replace one letter for another in test sequence.\n",
    "\n",
    "#how to replace one letter for each amino acid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutated_sequences = [] #creates a list to store the mutated amino acids\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"  # defines the standard 20 amino acids\n",
    "for i, original_aa in enumerate(testsequence): #indexes the original amino acids \n",
    "    for new_aa in amino_acids: #creates a loop of all amino acids\n",
    "        if new_aa != original_aa:  # Avoid replacing with itself\n",
    "                mutated_seq = testsequence[:i] + new_aa + testsequence[i+1:] #creates a new sequence (mutated seq)\n",
    "                mutated_sequences.append(mutated_seq) #stores mutated sequence\n",
    "\n",
    "print(f'Mutated sequences (# sequences = {len(mutated_sequences)})')\n",
    "print(mutated_sequences)\n",
    " #shows mutations at each position\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task2 - Run these sequences through the predict function and store the output as a csv file. Look at pandas.to_csv() for this.\n",
    "Output should be 280 separate csv files. Don't worry about a naming convention yet. \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kinase_library as kl\n",
    "import pandas as pd\n",
    "\n",
    "testsequence = 'PSVEPPLsQETFSDL'  \n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "mutated_sequences = []  \n",
    "\n",
    "for i, original_aa in enumerate(testsequence):  \n",
    "    for new_aa in amino_acids:\n",
    "        if new_aa != original_aa:  \n",
    "                mutated_seq = testsequence[:i] + new_aa + testsequence[i+1:] \n",
    "                mutated_sequences.append((i, original_aa, new_aa, mutated_seq))  \n",
    "\n",
    "\n",
    "print(f\"Total mutations generated: {len(mutated_sequences)}\") \n",
    "\n",
    "for i, original_aa, new_aa, mutated_seq in mutated_sequences: #below is gpt code, wanted to see what exceptions it threw up using try function\n",
    "    try:\n",
    "        s = kl.Substrate(mutated_seq)\n",
    "        predictions = s.predict()\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(predictions)\n",
    "\n",
    "        # Save to CSV file with a structured name\n",
    "        filename = f\"mutations/mutation_Pos{i}_{original_aa}_to_{new_aa}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping mutation at position {i} ({original_aa} â†’ {new_aa}): {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing mutation CSVs\n",
    "mut_predictions = \"mutations/\"\n",
    "\n",
    "# Find a sample mutation file to use as a template\n",
    "sample_file = None\n",
    "for file in os.listdir(mut_predictions):\n",
    "    if file.endswith(\".csv\"):\n",
    "        sample_file = os.path.join(mut_predictions, file)\n",
    "        break  # Stop after finding the first valid file\n",
    "\n",
    "if not sample_file:\n",
    "    raise ValueError(\"No mutation CSV files found!\")\n",
    "\n",
    "# Read sample file\n",
    "df = pd.read_csv(sample_file)\n",
    "\n",
    "# Ensure \"Score Rank\" exists\n",
    "if \"Score Rank\" not in df.columns:\n",
    "    raise ValueError(f\"'Score Rank' column not found in {sample_file}\")\n",
    "\n",
    "# Create a wild-type CSV using the same structure\n",
    "df_wildtype = df.copy()\n",
    "df_wildtype[\"Mutation\"] = \"Wild-Type\"  # Mark as wild-type\n",
    "\n",
    "# Save the new wild-type CSV\n",
    "wildtype_path = os.path.join(mut_predictions, \"wildtype.csv\")\n",
    "df_wildtype.to_csv(wildtype_path, index=False)\n",
    "\n",
    "print(f\"Wild-type CSV created: {wildtype_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task3 - Run through the 286 csv files and extract the order of the kinases. Then using the test sequence as the reference.\n",
    "Compute a metric for each pair using something similar to LCS (link for inspo attached - https://chatgpt.com/share/67c705a9-9c74-8001-a8da-eac57f06384f)  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "#This extracts the kinase orders from 269 files - one being the reference sequence.\n",
    "#I assume some mutations at position 7 are skipped due to the lack of phosphoacceptor.\n",
    "#So giving 18 fewer LCS scores compared to the number of CSV files generated earlier.\n",
    "#This is consistent with the data shown on the heatmap in the next cell.\n",
    "\n",
    "mut_predictions = \"mutations/\"\n",
    "\n",
    "wildtype_path = os.path.join(mut_predictions, \"wildtype.csv\")\n",
    "\n",
    "df_wildtype = pd.read_csv(wildtype_path)\n",
    "\n",
    "wildtype_kinase_order = df_wildtype[\"Score Rank\"].tolist()  #extracts kinase orders from the reference sequence\n",
    "\n",
    "kinase_orders = {}\n",
    "\n",
    "#reads all files to extract the kinase orders (except wildtype as that's already been done)\n",
    "for file in os.listdir(mut_predictions):\n",
    "    if file.endswith(\".csv\") and file != \"wildtype.csv\":\n",
    "        file_path = os.path.join(mut_predictions, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if \"Score Rank\" in df.columns:\n",
    "                kinase_orders[file] = df[\"Score Rank\"].tolist()  # Store kinase order\n",
    "            else:\n",
    "                print(f\"Skipping {file}: No 'Score Rank' column found.\") \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")   #debugging steps\n",
    "\n",
    "print(f\"Extracted kinase orders from {len(kinase_orders)} mutation CSVs.\") \n",
    "\n",
    "# Function to compute Longest Common Subsequence (LCS)\n",
    "def longest_common_subsequence(seq1, seq2):\n",
    "    matcher = SequenceMatcher(None, seq1, seq2)\n",
    "    return sum(block.size for block in matcher.get_matching_blocks())\n",
    "\n",
    "#finds the LCS between wildtype and mut seqs\n",
    "lcs_scores = {}\n",
    "for file, order in kinase_orders.items():\n",
    "    lcs_length = longest_common_subsequence(order, wildtype_kinase_order)\n",
    "    lcs_scores[file] = lcs_length\n",
    "\n",
    "#normalizes LCS scores between 0 and 1\n",
    "if lcs_scores:\n",
    "    lcs_min = min(lcs_scores.values())\n",
    "    lcs_max = max(lcs_scores.values())\n",
    "\n",
    "    normalized_lcs = {file: (score - lcs_min) / (lcs_max - lcs_min) for file, score in lcs_scores.items()}\n",
    "\n",
    "   \n",
    "    df_lcs = pd.DataFrame(list(normalized_lcs.items()), columns=['Mutation_File', 'Normalized_LCS'])\n",
    "    \n",
    "    print(\"\\nNormalized LCS Scores:\")\n",
    "    print(df_lcs.sort_values(by=\"Normalized_LCS\", ascending=True))  # Sort by lowest LCS\n",
    "\n",
    "else:\n",
    "    print(\"No LCS scores calculated. Ensure kinase_orders contains data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task 4: generate heatmaps of LCS ratio for each mutation on the y axis and position on the x axis. I like plotly heatmaps for this\n",
    "https://plotly.com/python/heatmaps/. So output will be a heat map showing hotspots for the mutations that disrup the phosphorylation motif the most\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "lcs_data = []\n",
    "\n",
    "for file, normalized_score in normalized_lcs.items():\n",
    "    \n",
    "    match = re.match(r\"mutation_Pos(\\d+)_\\w+_to_(\\w+)\\.csv\", file)\n",
    "\n",
    "    if match:\n",
    "        position = int(match.group(1)) \n",
    "        mutated_aa = match.group(2)  \n",
    "\n",
    "        lcs_data.append([position, mutated_aa, normalized_score])\n",
    "\n",
    "\n",
    "df_lcs = pd.DataFrame(lcs_data, columns=['Position', 'Mutation', 'LCS_Ratio'])\n",
    "\n",
    "\n",
    "heatmap_matrix = df_lcs.pivot_table(index='Mutation', columns='Position', values='LCS_Ratio')\n",
    "\n",
    "\n",
    "fig = px.imshow(\n",
    "    heatmap_matrix,\n",
    "    labels={'x': 'Mutation Position', 'y': 'Mutated Amino Acid', 'color': 'LCS Ratio'},\n",
    "    color_continuous_scale='Temps',\n",
    "    title=\"Heatmap of LCS Ratios for Each Mutation\",\n",
    ")\n",
    "\n",
    "# Format axes\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        tickmode='linear',\n",
    "        dtick=1  # Show every position\n",
    "    ),\n",
    "    coloraxis_colorbar=dict(\n",
    "        tickvals=[0, 0.25, 0.5, 0.75, 1],  # Set scale from 0 to 1\n",
    "        ticktext=[\"0.0\", \"0.25\", \"0.5\", \"0.75\", \"1.0\"]\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Generate tables and subplots showing cases for high and low LCS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from difflib import SequenceMatcher\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths and variables\n",
    "mut_predictions = \"C:/Users/ricea/OneDrive/Documents/GitHub/Annabel-BSc-Project\"\n",
    "kinase_orders = {}\n",
    "\n",
    "def extract_kinase_orders():\n",
    "    \"\"\"Reads CSV files and extracts kinase orders.\"\"\"\n",
    "    global kinase_orders\n",
    "    for file in os.listdir(mut_predictions):  # Finds the CSVs, reads the CSVs\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(mut_predictions, file)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Check if \"Score Rank\" column exists\n",
    "                if \"Score Rank\" in df.columns:\n",
    "                    kinase_list = df[\"Score Rank\"].tolist()\n",
    "                    kinase_orders[file] = kinase_list  # Store kinase order for each mutation\n",
    "                else:\n",
    "                    print(f\"Skipping {file}: No 'Score Rank' column found.\")  # Proofreads\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")  # Proofreads\n",
    "\n",
    "    print(f\"Extracted kinase orders from {len(kinase_orders)} CSV files.\")  # Checks if it's done\n",
    "\n",
    "def longest_common_subsequence(seq1, seq2):\n",
    "    \"\"\"Computes the Longest Common Subsequence (LCS) length between two lists.\"\"\"\n",
    "    matcher = SequenceMatcher(None, seq1, seq2)\n",
    "    return sum(block.size for block in matcher.get_matching_blocks())\n",
    "\n",
    "def calculate_lcs_scores():\n",
    "    \"\"\"Calculates the pairwise LCS scores between kinase orders.\"\"\"\n",
    "    pairwise_lcs = {}\n",
    "    for (file1, order1), (file2, order2) in itertools.combinations(kinase_orders.items(), 2):\n",
    "        lcs_length = longest_common_subsequence(order1, order2)\n",
    "        pairwise_lcs[(file1, file2)] = lcs_length\n",
    "\n",
    "    lcs_scores = list(pairwise_lcs.values())\n",
    "    lcs_min = min(lcs_scores)\n",
    "    lcs_max = max(lcs_scores)\n",
    "\n",
    "    # Prevent division by zero if all scores are the same\n",
    "    if lcs_max - lcs_min == 0:\n",
    "        print(\"All LCS scores are identical. Normalization skipped.\")\n",
    "        normalized_lcs = {pair: 1.0 for pair in pairwise_lcs}  # Assign all a score of 1\n",
    "    else:\n",
    "        normalized_lcs = {pair: (score - lcs_min) / (lcs_max - lcs_min) for pair, score in pairwise_lcs.items()}\n",
    "\n",
    "    return normalized_lcs\n",
    "\n",
    "def display_lcs_tables(normalized_lcs):\n",
    "    \"\"\"Displays side-by-side tables for LCS scores in specific ranges.\"\"\"\n",
    "   \n",
    "    lcs_groups = {\n",
    "        '0.7-1.0': [],\n",
    "        '0.000001-0.1': []\n",
    "    }\n",
    "\n",
    "    # Group the data by the LCS score ranges\n",
    "    for (pair, normalized_score) in normalized_lcs.items():\n",
    "        if 0.7 <= normalized_score <= 1.0:\n",
    "            lcs_groups['0.7-1.0'].append((pair, normalized_score))\n",
    "        elif 0.000001 <= normalized_score <= 0.1:\n",
    "            lcs_groups['0.000001-0.1'].append((pair, normalized_score))\n",
    "\n",
    "    # Create side-by-side tables for each LCS group\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "    for idx, (score_group, ax) in enumerate(zip(['0.7-1.0', '0.000001-0.1'], axes)):\n",
    "        data = lcs_groups[score_group][:2]\n",
    "        \n",
    "        # Only create a table if there is data\n",
    "        if data:\n",
    "            table_data = [(pair[0], pair[1], f\"{score:.4f}\") for pair, score in data]\n",
    "\n",
    "            # Create a table\n",
    "            table = ax.table(cellText=table_data,\n",
    "                            colLabels=[\"Kinase 1\", \"Kinase 2\", \"Normalized LCS Score\"],\n",
    "                            loc=\"center\",\n",
    "                            cellLoc=\"center\",\n",
    "                            colColours=[\"#f5f5f5\"] * 3)\n",
    "\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(f\"LCS Score: {score_group}\", fontsize=14)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f\"No data for LCS = {score_group}\",\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=12, color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example of using the functions:\n",
    "# Run this once to extract kinase orders and calculate LCS scores\n",
    "extract_kinase_orders()\n",
    "normalized_lcs = calculate_lcs_scores()\n",
    "\n",
    "# Then you can run the display function whenever you want to show the tables\n",
    "display_lcs_tables(normalized_lcs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task 5. Using function below. Get a workflow up and running to ping uniprot servers, extract protein sequence \n",
    "in the format needed by kinase library to run the code on extracted sequences. You'll need to import the request library using\n",
    "pip install requests\n",
    "'''\n",
    "\n",
    "!pip install requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def extract_genomic_information_from_uniprot_id(uniprot_id):\n",
    "    '''\n",
    "    Takes in uniprot ID as a string input and pings the Uniprot API to extract genomic coordinates of the protein and exons.\n",
    "    Metadata such as name, taxID, protein sequence, genome assembly name,  ENSEMBL GeneID, ENSEMBL Transcript ID and ENSEMBL Translations IDs is included alongside the extracted coordinates.  \n",
    "\n",
    "    Args:\n",
    "    uniprot_id (str): uniprot ID\n",
    "\n",
    "    Returns:\n",
    "    genomic_information (pd.DataFrame): DataFrame containing genomic coordinates of the protein of interest alongside exon positions and metadata \n",
    "    '''\n",
    "    genomic_information = pd.DataFrame()\n",
    "    try:\n",
    "        print(f'Searching for UniProt ID: {uniprot_id}')\n",
    "        requestURL_protein = f\"https://www.ebi.ac.uk/proteins/api/coordinates/{uniprot_id}\"\n",
    "        response_protein = requests.get(requestURL_protein, headers={\"Accept\": \"application/json\"})\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        response_protein.raise_for_status()\n",
    "        \n",
    "        # Load JSON response\n",
    "        response_protein = response_protein.json()\n",
    "        \n",
    "        # Check if response is not empty\n",
    "        if response_protein:\n",
    "            response_protein_normalise = pd.json_normalize(\n",
    "                response_protein, \n",
    "                record_path=['gnCoordinate', 'genomicLocation', 'exon'], \n",
    "                meta=['accession', 'name', 'taxid', 'sequence', \n",
    "                      ['gnCoordinate', 'genomicLocation', 'chromosome'], \n",
    "                      ['gnCoordinate', 'genomicLocation', 'start'], \n",
    "                      ['gnCoordinate', 'genomicLocation', 'end'], \n",
    "                      ['gnCoordinate', 'genomicLocation', 'reverseStrand'], \n",
    "                      ['gnCoordinate', 'genomicLocation', 'nucleotideId'], \n",
    "                      ['gnCoordinate', 'genomicLocation', 'assemblyName'], \n",
    "                      ['gnCoordinate', 'ensemblGeneId'], \n",
    "                      ['gnCoordinate', 'ensemblTranscriptId'], \n",
    "                      ['gnCoordinate', 'ensemblTranslationId']],\n",
    "                record_prefix='exon_'\n",
    "            )\n",
    "\n",
    "            # Group and aggregate exon information\n",
    "            response_protein_normalise = response_protein_normalise.groupby([\n",
    "                'accession', 'name', 'taxid', 'sequence', \n",
    "                'gnCoordinate.genomicLocation.chromosome', \n",
    "                'gnCoordinate.genomicLocation.start', \n",
    "                'gnCoordinate.genomicLocation.end', \n",
    "                'gnCoordinate.genomicLocation.reverseStrand', \n",
    "                'gnCoordinate.genomicLocation.nucleotideId', \n",
    "                'gnCoordinate.genomicLocation.assemblyName', \n",
    "                'gnCoordinate.ensemblGeneId', \n",
    "                'gnCoordinate.ensemblTranscriptId', \n",
    "                'gnCoordinate.ensemblTranslationId'\n",
    "            ]).agg({\n",
    "                'exon_id': lambda x: ','.join(map(str, x)),\n",
    "                'exon_proteinLocation.begin.position': lambda x: ','.join(map(str, x)),                    \n",
    "                'exon_proteinLocation.end.position': lambda x: ','.join(map(str, x)),\n",
    "                'exon_genomeLocation.begin.position': lambda x: ','.join(map(str, x)),                    \n",
    "                'exon_genomeLocation.end.position': lambda x: ','.join(map(str, x))\n",
    "            }).reset_index()\n",
    "\n",
    "            # Concatenate to the main DataFrame\n",
    "            genomic_information = pd.concat([genomic_information, response_protein_normalise], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No data found for UniProt ID: {uniprot_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return genomic_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example usage\n",
    "test_df = extract_genomic_information_from_uniprot_id('O15533')\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
